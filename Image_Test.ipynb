{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for classification\n",
    "\n",
    "Now you'll write a function to use a trained network for inference. That is, you'll pass an image into the network and predict the class of the flower in the image. Write a function called `predict` that takes an image and a model, then returns the top $K$ most likely classes along with the probabilities. It should look like \n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```\n",
    "\n",
    "First you'll need to handle processing the input image such that it can be used in your network. \n",
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "You'll want to use `PIL` to load the image ([documentation](https://pillow.readthedocs.io/en/latest/reference/Image.html)). It's best to write a function that preprocesses the image so it can be used as input for the model. This function should process the images in the same manner used for training. \n",
    "\n",
    "First, resize the images where the shortest side is 256 pixels, keeping the aspect ratio. This can be done with the [`thumbnail`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) or [`resize`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) methods. Then you'll need to crop out the center 224x224 portion of the image.\n",
    "\n",
    "Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1. You'll need to convert the values. It's easiest with a Numpy array, which you can get from a PIL image like so `np_image = np.array(pil_image)`.\n",
    "\n",
    "As before, the network expects the images to be normalized in a specific way. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`. You'll want to subtract the means from each color channel, then divide by the standard deviation. \n",
    "\n",
    "And finally, PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array. You can reorder dimensions using [`ndarray.transpose`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html). The color channel needs to be first and retain the order of the other two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, the function below converts a PyTorch tensor and displays it in the notebook. If your `process_image` function works, running the output through this function should return the original image (except for the cropped out portions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1 format:  JPEG\n",
      "image1 size:  (500, 601)\n",
      "image1 mode:  RGB\n"
     ]
    }
   ],
   "source": [
    "#for self test\n",
    "\n",
    "filepath1 = 'flowers/test/1/image_06743.jpg'\n",
    "image1 = Image.open(filepath1)\n",
    "\n",
    "print('image1 format: ',image1.format )\n",
    "print('image1 size: ',image1.size )\n",
    "print('image1 mode: ',image1.mode )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256,256)\n",
    "image1 = image1.resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1 format:  None\n",
      "image1 size:  (256, 256)\n",
      "image1 mode:  RGB\n"
     ]
    }
   ],
   "source": [
    "print('image1 format: ',image1.format )\n",
    "print('image1 size: ',image1.size )\n",
    "print('image1 mode: ',image1.mode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width:  256\n",
      "height:  256\n",
      "left:  16.0\n",
      "top:  16.0\n",
      "right:  240.0\n",
      "bottom:  240.0\n"
     ]
    }
   ],
   "source": [
    "width, height = image1.size\n",
    "print('width: ', width)\n",
    "print('height: ', height)\n",
    "new_width = 224\n",
    "new_height =224\n",
    "left = (width - new_width)/2\n",
    "top = (height - new_height)/2\n",
    "right = (width + new_width)/2\n",
    "bottom = (height + new_height)/2\n",
    "print('left: ',left)\n",
    "print('top: ',top)\n",
    "print('right: ',right)\n",
    "print('bottom: ',bottom)\n",
    "image1 = image1.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1 format:  None\n",
      "image1 size:  (224, 224)\n",
      "image1 mode:  RGB\n"
     ]
    }
   ],
   "source": [
    "print('image1 format: ',image1.format )\n",
    "print('image1 size: ',image1.size )\n",
    "print('image1 mode: ',image1.mode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image1 = np.array(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02352941,  0.05098039,  0.02352941],\n",
       "        [ 0.05098039,  0.07843137,  0.05098039],\n",
       "        [ 0.09019608,  0.10980392,  0.08627451],\n",
       "        ..., \n",
       "        [ 0.12941176,  0.18823529,  0.09803922],\n",
       "        [ 0.22745098,  0.30588235,  0.2       ],\n",
       "        [ 0.23921569,  0.31764706,  0.21960784]],\n",
       "\n",
       "       [[ 0.06666667,  0.09411765,  0.0627451 ],\n",
       "        [ 0.08235294,  0.10980392,  0.07843137],\n",
       "        [ 0.08235294,  0.10196078,  0.0745098 ],\n",
       "        ..., \n",
       "        [ 0.04705882,  0.0745098 ,  0.04313725],\n",
       "        [ 0.02352941,  0.07843137,  0.01176471],\n",
       "        [ 0.05490196,  0.10980392,  0.04313725]],\n",
       "\n",
       "       [[ 0.09803922,  0.1254902 ,  0.09411765],\n",
       "        [ 0.09019608,  0.11764706,  0.08627451],\n",
       "        [ 0.0627451 ,  0.08235294,  0.05490196],\n",
       "        ..., \n",
       "        [ 0.05490196,  0.07058824,  0.0745098 ],\n",
       "        [ 0.07843137,  0.11764706,  0.0745098 ],\n",
       "        [ 0.10980392,  0.15686275,  0.10196078]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.18431373,  0.24705882,  0.10588235],\n",
       "        [ 0.18039216,  0.25098039,  0.10196078],\n",
       "        [ 0.16862745,  0.25098039,  0.09019608],\n",
       "        ..., \n",
       "        [ 0.14509804,  0.23921569,  0.01176471],\n",
       "        [ 0.14117647,  0.24313725,  0.        ],\n",
       "        [ 0.13333333,  0.22745098,  0.        ]],\n",
       "\n",
       "       [[ 0.16862745,  0.23921569,  0.09019608],\n",
       "        [ 0.15294118,  0.23529412,  0.0745098 ],\n",
       "        [ 0.17254902,  0.2627451 ,  0.09019608],\n",
       "        ..., \n",
       "        [ 0.11372549,  0.21568627,  0.        ],\n",
       "        [ 0.11372549,  0.21960784,  0.        ],\n",
       "        [ 0.11764706,  0.21960784,  0.        ]],\n",
       "\n",
       "       [[ 0.1254902 ,  0.20392157,  0.01176471],\n",
       "        [ 0.11372549,  0.18823529,  0.00784314],\n",
       "        [ 0.09019608,  0.16470588,  0.        ],\n",
       "        ..., \n",
       "        [ 0.10980392,  0.19607843,  0.01568627],\n",
       "        [ 0.11372549,  0.19215686,  0.        ],\n",
       "        [ 0.10980392,  0.18823529,  0.        ]]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = np_image1/255\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "#transform numpy array to torch tensor\n",
    "image1_normalized = (image1 - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.01515541, -1.80812325, -1.69986928],\n",
       "        [-1.89528213, -1.68557423, -1.57786492],\n",
       "        [-1.72403459, -1.54551821, -1.42100218],\n",
       "        ..., \n",
       "        [-1.55278705, -1.19537815, -1.3687146 ],\n",
       "        [-1.12466821, -0.67016807, -0.91555556],\n",
       "        [-1.07329395, -0.61764706, -0.82840959]],\n",
       "\n",
       "       [[-1.82678311, -1.61554622, -1.52557734],\n",
       "        [-1.7582841 , -1.54551821, -1.45586057],\n",
       "        [-1.7582841 , -1.58053221, -1.47328976],\n",
       "        ..., \n",
       "        [-1.91240688, -1.70308123, -1.61272331],\n",
       "        [-2.01515541, -1.68557423, -1.75215686],\n",
       "        [-1.87815738, -1.54551821, -1.61272331]],\n",
       "\n",
       "       [[-1.68978508, -1.4754902 , -1.38614379],\n",
       "        [-1.72403459, -1.5105042 , -1.42100218],\n",
       "        [-1.84390787, -1.66806723, -1.56043573],\n",
       "        ..., \n",
       "        [-1.87815738, -1.72058824, -1.47328976],\n",
       "        [-1.77540885, -1.5105042 , -1.47328976],\n",
       "        [-1.63841082, -1.33543417, -1.3512854 ]],\n",
       "\n",
       "       ..., \n",
       "       [[-1.3130405 , -0.93277311, -1.33385621],\n",
       "        [-1.33016525, -0.91526611, -1.3512854 ],\n",
       "        [-1.38153952, -0.91526611, -1.40357298],\n",
       "        ..., \n",
       "        [-1.48428804, -0.96778711, -1.75215686],\n",
       "        [-1.50141279, -0.95028011, -1.80444444],\n",
       "        [-1.5356623 , -1.02030812, -1.80444444]],\n",
       "\n",
       "       [[-1.38153952, -0.96778711, -1.40357298],\n",
       "        [-1.45003853, -0.98529412, -1.47328976],\n",
       "        [-1.36441476, -0.8627451 , -1.40357298],\n",
       "        ..., \n",
       "        [-1.62128607, -1.07282913, -1.80444444],\n",
       "        [-1.62128607, -1.05532213, -1.80444444],\n",
       "        [-1.60416132, -1.05532213, -1.80444444]],\n",
       "\n",
       "       [[-1.56991181, -1.12535014, -1.75215686],\n",
       "        [-1.62128607, -1.19537815, -1.76958606],\n",
       "        [-1.72403459, -1.30042017, -1.80444444],\n",
       "        ..., \n",
       "        [-1.63841082, -1.16036415, -1.73472767],\n",
       "        [-1.62128607, -1.17787115, -1.80444444],\n",
       "        [-1.63841082, -1.19537815, -1.80444444]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
